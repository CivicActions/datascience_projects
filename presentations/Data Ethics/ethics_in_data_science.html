<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Data Ethics</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jake Rozran" />
    <meta name="date" content="2022-05-06" />
    <script src="ethics_in_data_science_files/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Data Ethics
]
.subtitle[
## Lunch &amp; Learn
]
.author[
### Jake Rozran
]
.institute[
### CivicActions
]
.date[
### May 6, 2022
]

---

class: center, middle





<div>
<style type="text/css">.xaringan-extra-logo {
width: 110px;
height: 128px;
z-index: 0;
background-image: url(ethics_in_data_science_files/CA-Full-Logo-Red-1292x204.png);
background-size: contain;
background-repeat: no-repeat;
position: absolute;
bottom:-4.5em;left:1em;
}
</style>
<script>(function () {
  let tries = 0
  function addLogo () {
    if (typeof slideshow === 'undefined') {
      tries += 1
      if (tries < 10) {
        setTimeout(addLogo, 100)
      }
    } else {
      document.querySelectorAll('.remark-slide-content:not(.hide_logo)')
        .forEach(function (slide) {
          const logo = document.createElement('a')
          logo.classList = 'xaringan-extra-logo'
          logo.href = 'https://www.govern4america.org/'
          slide.appendChild(logo)
        })
    }
  }
  document.addEventListener('DOMContentLoaded', addLogo)
})()</script>
</div>

# Lying with Statistics

"Figures often beguile me, particularly when I have the arranging of them myself; in which case the remark attributed to [then British Prime Minister, Benjamin] Disraeli would often apply with justice and force: 'There are three kinds of lies: lies, damned lies, and statistics.'" - Mark Twain, 1907

---

class: center, middle

# Data Visualization can often be Manipulated for Purpose

---

class: center, middle

# Consider the following line chart. What conclusion do you draw?

![stand your ground](ethics_in_data_science_files/stand_your_ground.png)

---

class: center, middle

# Take a Closer Look at the y-axis

What is really happening?

![stand your ground](ethics_in_data_science_files/stand_your_ground.png)

---

class: center, middle

![shame](ethics_in_data_science_files/shame.gif)

---

class: center, middle

![bad_y_axis](ethics_in_data_science_files/temp_change.png)

---

class: center, middle

# Everything Looks Small from Far Away

![zoom_out](ethics_in_data_science_files/zoom_out.gif)

---

class: center, middle

# COVID is Getting Better?

Look at the dates closely... 

![COVID](ethics_in_data_science_files/covid-ga-recreation.jpeg)

---

class: center, middle

![oh_no](ethics_in_data_science_files/i_dont_like_that.gif)

---

class: center, middle

# Some Ethical Conundrums

---

class: left, middle

# Predicting Sexuality

- Y. Wang and Kosinski (2018) used machine learning to build a model that 
predicts sexual orientation based on pictures of people’s faces
- The authors claim that if given **five images of a person’s face**, their 
model would **correctly predict the sexual orientation** of 91% of men and 83% of 
women

---

class: left, middle

# Predicting Sexuality (continued)

1. Was this research ethical? 
2. Were the authors justified in creating and publishing this model?

---

class: left, middle

# Predicting Sexuality (continued)

- The authors highlight the potential harm that their work could do in their 
abstract:
&gt; "Additionally, given that companies and governments are increasingly using computer vision algorithms to detect people’s intimate traits, our findings expose a threat to the privacy and safety of gay men and women."

---

class: left, middle

# Predicting Sexuality (continued)

1. Now was this research ethical? 
2. Does this change your opinion on the justification?

---

class: left, middle

# Predicting Sexuality (continued)

A subsequent article in *The New Yorker* also notes that:
&gt; "the study consisted entirely of white faces, but only because the dating site had served up too few faces of color to provide for meaningful analysis."

---

class: left, middle

# Predicting Sexuality (continued)

1. Now we have more context... how do we feel about the ethics?
2. Were the authors justified in creating and publishing this model?

---

class: left, middle

# Predicting Race

- Imai and Khanna (2016) built a racial prediction algorithm using machine 
learning 
- The model was trained using voter registration records from Florida and the 
U.S. Census Bureau’s name list
- The model returns predicted probabilities for a person’s race based on either 
their last name alone, or their last name and their address

---

class: left, middle

# Predicting Race (continued)

Was the publication of this model ethical? 

---

class: left, middle

# Predicting Race (continued)

In addition to the publishing the paper detailing the methodology, the authors 
published the software for the classifier on GitHub under an open-source license

---

class: left, middle

# Predicting Race (continued)

1. Was the publication of this model ethical? 
2. Does the open-source nature of the code affect your answer? 
3. Is it ethical to use this software? 
4. Does your answer change depending on the intended use?


---

class: center, middle

# Algorithmic Bias

![algo_bias](ethics_in_data_science_files/reproducibility_court_blank.png)

---

class: left, middle

# Algorithmic Bias

- Algorithms are at the core of many data science models
- Biased data may lead to algorithmic bias
- For example: Some groups may be underrepresented or systematically excluded 
from data collection efforts

---

class: left, middle

# Algorithmic Bias Example

- Consider a criminal recidivism algorithm used in several states and detailed in 
a ProPublica story titled “Machine Bias” (Angwin et al. 2016)
- The algorithm returns predictions about how likely a criminal is to commit 
another crime based on a survey of 137 questions

ProPublica claims that the algorithm is biased:

&gt; "Black defendants were still 77 percent more likely to be pegged as at higher risk of committing a future violent crime and 45 percent more likely to be predicted to commit a future crime of any kind."

---

class: center, middle

# Algorithmic Bias Example (continued)

How could the predictions be biased, when the race of the defendants is not 
included in the model?

---

class: left, middle

# Algorithmic Bias Example (continued)

Consider that one of the survey questions is "was one of your parents ever sent 
to jail or prison?"

**Proxy Variable**: a variable that is not in itself directly relevant, but that 
serves in place of an unobservable or immeasurable variable

---

class: left, middle

# Podcast Discussion

1. What shocked you the most from the podcast?
2. Are there places where facial recognition may be beneficial?
3. What do you think about consent and facial recognition?
4. How do we fix the issues from the podcast?

---

class: left, middle

# Additional Resources

.pull-left[
Some great books outlining the issue(s):

- [Weapons of Math Destruction](https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality-ebook/dp/B019B6VCLO/)
- [Algorithms of Oppression](https://www.amazon.com/Algorithms-Oppression-Search-Engines-Reinforce-ebook/dp/B075XS7Y7D/ref=sr_1_1)
- [Artificial Unintelligence](https://www.amazon.com/Artificial-Unintelligence-Computers-Misunderstand-World-ebook/dp/B08BT23822/ref=sr_1_1)
- [Artificial Intelligence](https://www.amazon.com/Artificial-Intelligence-Guide-Thinking-Humans-ebook/dp/B07MYWPQSK/ref=sr_1_1)
- [Superintelligence](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom-ebook/dp/B00LOOCGB2/ref=sr_1_1)
- [The Hype Machine](https://www.amazon.com/Hype-Machine-Disrupts-Elections-Health-ebook/dp/B083RZKJY3/ref=sr_1_1)
- [The Alignment Problem](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values-ebook/dp/B085T55LGK/ref=sr_1_2)
- [The Loop](https://www.amazon.com/Loop-Technology-Creating-Without-Choices-ebook/dp/B093ZQ5ZWX/ref=sr_1_1)
]

.pull-right[
One book that I've found that gets a bit into the potential solutions:

- [The Ethical Algorithm](https://www.amazon.com/Ethical-Algorithm-Science-Socially-Design-ebook/dp/B07XLTXBXV/ref=sr_1_1)

Podcasts:

- [My, Myself, and AI](https://sloanreview.mit.edu/audio-series/me-myself-and-ai/)
- [Your Undivided Attention](https://www.humanetech.com/podcast)
- [In Machines We Trust](https://forms.technologyreview.com/in-machines-we-trust/)

Documentaries:

- [Coded Bias](https://www.codedbias.com/)
- [The Social Network](https://www.netflix.com/title/70132721)
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
